{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset  \n",
    "from peft import LoraConfig, get_peft_model \n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 定义模型加载的设备\n",
    "device = \"cuda\"\n",
    "\n",
    "# 加载预训练的语言模型，自动适配数据类型并映射到指定设备\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"C:/Users/Lenovo/Desktop/Lora微调/Qwen2.5-0.5B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 加载与模型对应的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"C:/Users/Lenovo/Desktop/Lora微调/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tokenizer, batch_messages):\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    \n",
    "    im_start = tokenizer('<|im_start|>').input_ids\n",
    "    im_end = tokenizer('<|im_end|>').input_ids\n",
    "    newline = tokenizer('\\n').input_ids\n",
    "    pad_token = tokenizer.pad_token_id  # 获取 pad token 的 ID\n",
    "    ignore = [-100]\n",
    "    \n",
    "    for group in batch_messages:\n",
    "        role = tokenizer(f\"user\\n{group['instruction']}\").input_ids\n",
    "        content = tokenizer(f\"assistant\\n{group['output']}\").input_ids\n",
    "        \n",
    "        input_ids = im_start + role + im_end + newline\n",
    "        target_ids = im_start + ignore * len(role) + content + im_end + newline\n",
    "        \n",
    "        input_list.append(input_ids)\n",
    "        target_list.append(target_ids)\n",
    "    \n",
    "    # 计算最大长度，并确保所有序列具有相同长度\n",
    "    max_len = max([len(ids) for ids in input_list])\n",
    "    \n",
    "    padded_input_list = []\n",
    "    padded_target_list = []\n",
    "    \n",
    "    for input_ids, target_ids in zip(input_list, target_list):\n",
    "        # 确保精确填充到 max_len\n",
    "        padded_input = input_ids[:max_len] if len(input_ids) > max_len else input_ids + [pad_token] * (max_len - len(input_ids))\n",
    "        padded_target = target_ids[:max_len] if len(target_ids) > max_len else target_ids + ignore * (max_len - len(target_ids))\n",
    "        \n",
    "        padded_input_list.append(padded_input)\n",
    "        padded_target_list.append(padded_target)\n",
    "    \n",
    "    batch_input_ids = torch.tensor(padded_input_list, dtype=torch.long)\n",
    "    batch_target_ids = torch.tensor(padded_target_list, dtype=torch.long)\n",
    "    \n",
    "    # 明确设置 attention_mask\n",
    "    batch_mask = (batch_input_ids != pad_token).long()\n",
    "    \n",
    "    return batch_input_ids, batch_target_ids, batch_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_path = 'C:/Users/Lenovo/Desktop/Lora微调/黛玉风.json'\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        datas = json.load(f)\n",
    "    return datas\n",
    "\n",
    "datas = read_json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model  \n",
    "\n",
    "# 配置 LoRA  \n",
    "lora_config = LoraConfig(  \n",
    "    r=8,  # LoRA的秩，影响参数量  \n",
    "    lora_alpha=32,  # LoRA alpha系数  \n",
    "    lora_dropout=0.1,  # Dropout率，用于正则化  \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # 模块名称  \n",
    "    task_type=\"CAUSAL_LM\",  # 任务类型  \n",
    ")  \n",
    "\n",
    "# 将 LoRA 应用到 Qwen 模型  \n",
    "model = get_peft_model(model, lora_config)  \n",
    "\n",
    "# 查看可训练参数  \n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0轮训练\n",
      "训练了第0次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.9629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第1轮训练\n",
      "训练了第1次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.6522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第2轮训练\n",
      "训练了第2次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.9366, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第3轮训练\n",
      "训练了第3次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(4.4334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第4轮训练\n",
      "训练了第4次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.9399, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第5轮训练\n",
      "训练了第5次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.5149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第6轮训练\n",
      "训练了第6次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.4553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第7轮训练\n",
      "训练了第7次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(4.1059, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第8轮训练\n",
      "训练了第8次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(4.8652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第9轮训练\n",
      "训练了第9次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(4.4861, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第10轮训练\n",
      "训练了第10次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(4.2346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第11轮训练\n",
      "训练了第11次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.9591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第12轮训练\n",
      "训练了第12次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.6869, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第13轮训练\n",
      "训练了第13次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.4831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第14轮训练\n",
      "训练了第14次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.9699, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第15轮训练\n",
      "训练了第15次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(4.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第16轮训练\n",
      "训练了第16次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.9502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第17轮训练\n",
      "训练了第17次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.7253, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第18轮训练\n",
      "训练了第18次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(4.0902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第19轮训练\n",
      "训练了第19次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.6089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第20轮训练\n",
      "训练了第20次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.7275, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第21轮训练\n",
      "训练了第21次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(4.1469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第22轮训练\n",
      "训练了第22次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.4956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第23轮训练\n",
      "训练了第23次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.5242, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第24轮训练\n",
      "训练了第24次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(4.2519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第25轮训练\n",
      "训练了第25次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.5967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第26轮训练\n",
      "训练了第26次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.1860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第27轮训练\n",
      "训练了第27次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.4115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第28轮训练\n",
      "训练了第28次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.9319, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第29轮训练\n",
      "训练了第29次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第30轮训练\n",
      "训练了第30次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.2600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第31轮训练\n",
      "训练了第31次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.2662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第32轮训练\n",
      "训练了第32次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.9229, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第33轮训练\n",
      "训练了第33次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.3672, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第34轮训练\n",
      "训练了第34次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.4440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第35轮训练\n",
      "训练了第35次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(4.3794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第36轮训练\n",
      "训练了第36次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.6176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第37轮训练\n",
      "训练了第37次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.0114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第38轮训练\n",
      "训练了第38次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.0559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第39轮训练\n",
      "训练了第39次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第40轮训练\n",
      "训练了第40次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.6645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第41轮训练\n",
      "训练了第41次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.2839, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第42轮训练\n",
      "训练了第42次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.6497, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第43轮训练\n",
      "训练了第43次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.2128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第44轮训练\n",
      "训练了第44次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.2948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第45轮训练\n",
      "训练了第45次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.4785, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第46轮训练\n",
      "训练了第46次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.4680, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第47轮训练\n",
      "训练了第47次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.3574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第48轮训练\n",
      "训练了第48次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(4.1622, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第49轮训练\n",
      "训练了第49次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.1122, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第50轮训练\n",
      "训练了第50次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.0296, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第51轮训练\n",
      "训练了第51次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.8502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第52轮训练\n",
      "训练了第52次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.9077, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第53轮训练\n",
      "训练了第53次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.5196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第54轮训练\n",
      "训练了第54次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(4.4794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第55轮训练\n",
      "训练了第55次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.8082, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第56轮训练\n",
      "训练了第56次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.2605, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第57轮训练\n",
      "训练了第57次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.9516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第58轮训练\n",
      "训练了第58次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.4667, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第59轮训练\n",
      "训练了第59次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(4.2107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第60轮训练\n",
      "训练了第60次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.4444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第61轮训练\n",
      "训练了第61次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.8325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第62轮训练\n",
      "训练了第62次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.7318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第63轮训练\n",
      "训练了第63次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.1968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第64轮训练\n",
      "训练了第64次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.2113, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第65轮训练\n",
      "训练了第65次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.1194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第66轮训练\n",
      "训练了第66次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.1994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第67轮训练\n",
      "训练了第67次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.6502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第68轮训练\n",
      "训练了第68次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.8805, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第69轮训练\n",
      "训练了第69次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.3955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第70轮训练\n",
      "训练了第70次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.3736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第71轮训练\n",
      "训练了第71次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.6021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第72轮训练\n",
      "训练了第72次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.7225, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第73轮训练\n",
      "训练了第73次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.6120, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第74轮训练\n",
      "训练了第74次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.1629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第75轮训练\n",
      "训练了第75次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.0297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第76轮训练\n",
      "训练了第76次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.4117, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第77轮训练\n",
      "训练了第77次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.8558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第78轮训练\n",
      "训练了第78次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.7535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第79轮训练\n",
      "训练了第79次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.4320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第80轮训练\n",
      "训练了第80次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.5399, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第81轮训练\n",
      "训练了第81次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第82轮训练\n",
      "训练了第82次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.4517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第83轮训练\n",
      "训练了第83次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.5960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第84轮训练\n",
      "训练了第84次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.5686, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第85轮训练\n",
      "训练了第85次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.3284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第86轮训练\n",
      "训练了第86次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.5772, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第87轮训练\n",
      "训练了第87次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.9677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第88轮训练\n",
      "训练了第88次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.1233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第89轮训练\n",
      "训练了第89次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.9044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第90轮训练\n",
      "训练了第90次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.5934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第91轮训练\n",
      "训练了第91次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.6173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第92轮训练\n",
      "训练了第92次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.9700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第93轮训练\n",
      "训练了第93次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.2539, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第94轮训练\n",
      "训练了第94次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.4844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第95轮训练\n",
      "训练了第95次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.5898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第96轮训练\n",
      "训练了第96次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(3.6205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第97轮训练\n",
      "训练了第97次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第98轮训练\n",
      "训练了第98次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.3000, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "第99轮训练\n",
      "训练了第99次\n",
      "logits: torch.Size([56, 22, 151936])\n",
      "targets: torch.Size([56, 22])\n",
      "loss: tensor(2.7537, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for i in range(100):\n",
    "    print(\"第{}轮训练\".format(i))\n",
    "    batch_input_ids,batch_target_ids,batch_mask=preprocess(tokenizer,datas)\n",
    "    model_outputs=model(batch_input_ids.to(device))\n",
    "\n",
    "    output_tokens=model_outputs.logits.argmax(dim=-1)\n",
    "\n",
    "    logits=model_outputs.logits[:,:-1,:]\n",
    "    targets=batch_target_ids[:,1:].to(device)\n",
    "    print(\"训练了第{}次\".format(i))\n",
    "    print('logits:',logits.shape) # 模型输出\n",
    "    print('targets:',targets.shape) # 拟合目标\n",
    "\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "\n",
    "    # 损失\n",
    "    loss_fn=CrossEntropyLoss()\n",
    "    loss=loss_fn(logits.reshape(-1,logits.size(2)),targets.reshape(-1))\n",
    "    print('loss:',loss)\n",
    "\n",
    "    # 优化器\n",
    "    optimizer=torch.optim.SGD(model.parameters())\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 求梯度\n",
    "    loss.backward()\n",
    "\n",
    "    # 梯度下降\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt):\n",
    "    # 预设角色背景\n",
    "    system_prompt = \"你是林黛玉，贾宝玉的表妹。\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # 生成配置\n",
    "    generation_config = {\n",
    "        \"max_new_tokens\": 128,  # 控制输出长度\n",
    "        \"temperature\": 0.7,     # 控制随机性\n",
    "        \"top_p\": 0.9,           # 核采样\n",
    "        \"repetition_penalty\": 1.2  # 降低重复\n",
    "    }\n",
    "    \n",
    "    # 模型交互\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        **generation_config\n",
    "    )\n",
    "    \n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我素来不喜食这些外国货物，只是知道他们那里的茶比咱们的好吃些...你们小心点儿罢！这茶既不是咱们喝得着的，自然不好糟蹋他......但我不肯去拿。你只管慢慢品罢，我也愁不得吃东西……我原也想做些吃的；只是人家不曾晓得我的底细....我原在大观园里住惯了；如今要作些料理粗具奉养姑奶奶，又怕年月太紧，不便下厨……可是在外面受尊贵人家接见如此隆重的大礼，岂是'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=\"这是暹罗进贡的茶叶\"\n",
    "chat(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-robot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
